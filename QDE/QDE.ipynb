{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'question_text': [\n",
    "        'Explain the concept of time complexity in algorithms.',\n",
    "        'What is a binary search tree and how does it work?',\n",
    "        'Write a code snippet in Python to reverse a list.',\n",
    "        'Discuss the importance of data normalization in databases.',\n",
    "        'What is the Big O notation?',\n",
    "        'Compare and contrast SQL and NoSQL databases.',\n",
    "    ],\n",
    "    'difficulty_label': [3, 4, 2, 5, 3, 4]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['question_text'], df['difficulty_label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf_train = tfidf.fit_transform(X_train)\n",
    "X_tfidf_test = tfidf.transform(X_test)\n",
    "\n",
    "lr_model_tfidf = LinearRegression()\n",
    "lr_model_tfidf.fit(X_tfidf_train, y_train)\n",
    "lr_predictions_tfidf = lr_model_tfidf.predict(X_tfidf_test)\n",
    "lr_mse_tfidf = mean_squared_error(y_test, lr_predictions_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)  # Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up GPU/CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(750):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        predicted = torch.argmax(logits, dim=1)\n",
    "        labels = batch['labels']\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = total_correct / total_samples\n",
    "print(f\"Accuracy on test set: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate JSON documentation using OpenAI GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_json_documentation(topic):\n",
    "    prompt = f\"Generate JSON documentation for the topic: {topic}\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-002\",  # You can adjust the engine based on your preference\n",
    "        prompt=prompt,\n",
    "        max_tokens=500,  # Adjust the maximum number of tokens based on your needs\n",
    "        n=1,\n",
    "        stop=None,\n",
    "    )\n",
    "    generated_doc = response['choices'][0]['text']\n",
    "    return generated_doc\n",
    "\n",
    "# Example usage\n",
    "topic_input = \"Explain Higher Order Classification in React development\"\n",
    "json_documentation = generate_json_documentation(topic_input)\n",
    "\n",
    "print(f\"Question: {topic_input}\")\n",
    "print(f\"Generated JSON documentation:\\n{json_documentation}\")\n",
    "\n",
    "# Comparison of models using metrics\n",
    "from sklearn.metrics import mean_squared_error, f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "# Assuming lr_predictions_tfidf and bert_predictions are available\n",
    "lr_mse_tfidf = mean_squared_error(y_test, lr_predictions_tfidf)\n",
    "bert_f1 = f1_score(y_test, bert_predictions)\n",
    "bert_roc_auc = roc_auc_score(y_test, bert_predictions)\n",
    "bert_accuracy = accuracy_score(y_test, bert_predictions)\n",
    "\n",
    "print(f\"TF-IDF Mean Squared Error: {lr_mse_tfidf}\")\n",
    "print(f\"BERT F1 Score: {bert_f1}\")\n",
    "print(f\"BERT ROC AUC Score: {bert_roc_auc}\")\n",
    "print(f\"BERT Accuracy: {bert_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing metrics (ROC curve, confusion matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, bert_predictions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, bert_predictions)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, \n",
    "            xticklabels=[\"Easy\", \"Hard\"], yticklabels=[\"Easy\", \"Hard\"])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_rep = classification_report(y_true, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
